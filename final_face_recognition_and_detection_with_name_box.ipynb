{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(55, 196, 130, 121)]\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc09620b790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[{'box': [119, 43, 72, 91], 'confidence': 0.9999998807907104, 'keypoints': {'left_eye': (139, 76), 'right_eye': (174, 77), 'nose': (156, 97), 'mouth_left': (142, 112), 'mouth_right': (169, 113)}}]\n",
      "[(66, 163, 129, 100)]\n",
      "[{'box': [104, 57, 57, 73], 'confidence': 0.999996542930603, 'keypoints': {'left_eye': (122, 86), 'right_eye': (150, 86), 'nose': (136, 100), 'mouth_left': (125, 114), 'mouth_right': (146, 113)}}]\n",
      "[(66, 163, 129, 100)]\n",
      "[{'box': [104, 58, 55, 71], 'confidence': 1.0, 'keypoints': {'left_eye': (121, 84), 'right_eye': (147, 84), 'nose': (136, 99), 'mouth_left': (125, 113), 'mouth_right': (144, 113)}}]\n",
      "[(66, 170, 129, 107)]\n",
      "[{'box': [103, 55, 60, 75], 'confidence': 0.999996542930603, 'keypoints': {'left_eye': (123, 86), 'right_eye': (151, 87), 'nose': (138, 101), 'mouth_left': (126, 114), 'mouth_right': (147, 115)}}]\n",
      "[(66, 170, 129, 107)]\n",
      "[{'box': [107, 54, 58, 75], 'confidence': 0.9999996423721313, 'keypoints': {'left_eye': (127, 84), 'right_eye': (154, 85), 'nose': (142, 99), 'mouth_left': (129, 113), 'mouth_right': (150, 113)}}]\n",
      "[(66, 176, 129, 114)]\n",
      "[{'box': [115, 57, 57, 70], 'confidence': 0.999987006187439, 'keypoints': {'left_eye': (134, 83), 'right_eye': (162, 85), 'nose': (148, 97), 'mouth_left': (136, 112), 'mouth_right': (157, 112)}}]\n",
      "[(66, 170, 129, 107)]\n",
      "[{'box': [106, 55, 57, 74], 'confidence': 0.9999980926513672, 'keypoints': {'left_eye': (123, 86), 'right_eye': (150, 84), 'nose': (137, 100), 'mouth_left': (127, 115), 'mouth_right': (148, 113)}}]\n",
      "[(66, 163, 129, 100)]\n",
      "[{'box': [105, 58, 58, 73], 'confidence': 0.9999997615814209, 'keypoints': {'left_eye': (122, 86), 'right_eye': (149, 85), 'nose': (136, 101), 'mouth_left': (126, 115), 'mouth_right': (147, 114)}}]\n",
      "[(66, 183, 129, 121)]\n",
      "[{'box': [117, 59, 58, 71], 'confidence': 0.9999895095825195, 'keypoints': {'left_eye': (137, 86), 'right_eye': (165, 87), 'nose': (152, 102), 'mouth_left': (138, 115), 'mouth_right': (160, 116)}}]\n",
      "[(73, 183, 135, 121)]\n",
      "[{'box': [124, 62, 58, 74], 'confidence': 0.9999866485595703, 'keypoints': {'left_eye': (144, 90), 'right_eye': (171, 92), 'nose': (159, 107), 'mouth_left': (145, 119), 'mouth_right': (166, 121)}}]\n",
      "[(80, 197, 142, 135)]\n",
      "[{'box': [136, 66, 56, 71], 'confidence': 0.9999890327453613, 'keypoints': {'left_eye': (154, 94), 'right_eye': (181, 95), 'nose': (167, 111), 'mouth_left': (155, 121), 'mouth_right': (176, 121)}}]\n",
      "[(78, 193, 130, 141)]\n",
      "[{'box': [140, 67, 54, 70], 'confidence': 0.9999830722808838, 'keypoints': {'left_eye': (158, 92), 'right_eye': (183, 93), 'nose': (171, 109), 'mouth_left': (158, 119), 'mouth_right': (180, 120)}}]\n",
      "[(73, 190, 135, 128)]\n",
      "[{'box': [133, 64, 54, 66], 'confidence': 0.9998629093170166, 'keypoints': {'left_eye': (149, 88), 'right_eye': (173, 89), 'nose': (161, 103), 'mouth_left': (150, 113), 'mouth_right': (168, 114)}}]\n",
      "[(53, 190, 115, 128)]\n",
      "[{'box': [129, 39, 58, 75], 'confidence': 0.9998568296432495, 'keypoints': {'left_eye': (144, 69), 'right_eye': (173, 69), 'nose': (157, 87), 'mouth_left': (148, 100), 'mouth_right': (169, 100)}}]\n",
      "[(46, 190, 108, 128)]\n",
      "[{'box': [127, 34, 60, 78], 'confidence': 0.9999984502792358, 'keypoints': {'left_eye': (144, 65), 'right_eye': (172, 65), 'nose': (157, 80), 'mouth_left': (147, 95), 'mouth_right': (168, 95)}}]\n",
      "[(53, 190, 115, 128)]\n",
      "[{'box': [128, 35, 59, 79], 'confidence': 0.9999973773956299, 'keypoints': {'left_eye': (144, 66), 'right_eye': (172, 66), 'nose': (157, 82), 'mouth_left': (147, 97), 'mouth_right': (168, 97)}}]\n",
      "[(53, 190, 115, 128)]\n",
      "[{'box': [136, 40, 51, 72], 'confidence': 0.9999234676361084, 'keypoints': {'left_eye': (150, 68), 'right_eye': (175, 70), 'nose': (161, 83), 'mouth_left': (152, 97), 'mouth_right': (170, 98)}}]\n",
      "[(53, 197, 115, 135)]\n",
      "[{'box': [137, 43, 54, 68], 'confidence': 0.9999823570251465, 'keypoints': {'left_eye': (151, 67), 'right_eye': (177, 69), 'nose': (162, 81), 'mouth_left': (152, 96), 'mouth_right': (173, 97)}}]\n",
      "[(53, 197, 115, 135)]\n",
      "[{'box': [136, 42, 55, 69], 'confidence': 0.9999926090240479, 'keypoints': {'left_eye': (151, 68), 'right_eye': (177, 69), 'nose': (163, 82), 'mouth_left': (153, 96), 'mouth_right': (174, 97)}}]\n",
      "[(53, 197, 115, 135)]\n",
      "[{'box': [136, 42, 54, 69], 'confidence': 0.9999985694885254, 'keypoints': {'left_eye': (150, 68), 'right_eye': (177, 69), 'nose': (162, 82), 'mouth_left': (153, 96), 'mouth_right': (173, 97)}}]\n",
      "[(53, 197, 115, 135)]\n",
      "[{'box': [138, 42, 54, 72], 'confidence': 0.9999810457229614, 'keypoints': {'left_eye': (153, 69), 'right_eye': (180, 70), 'nose': (166, 84), 'mouth_left': (155, 97), 'mouth_right': (175, 98)}}]\n",
      "[(66, 197, 129, 135)]\n",
      "[{'box': [140, 56, 53, 67], 'confidence': 0.999077320098877, 'keypoints': {'left_eye': (155, 82), 'right_eye': (183, 84), 'nose': (168, 103), 'mouth_left': (156, 112), 'mouth_right': (177, 113)}}]\n",
      "[]\n",
      "[{'box': [132, 62, 65, 74], 'confidence': 0.9999692440032959, 'keypoints': {'left_eye': (153, 95), 'right_eye': (181, 95), 'nose': (167, 116), 'mouth_left': (155, 125), 'mouth_right': (179, 125)}}]\n",
      "[]\n",
      "[{'box': [134, 66, 67, 77], 'confidence': 0.9999253749847412, 'keypoints': {'left_eye': (154, 97), 'right_eye': (186, 97), 'nose': (170, 122), 'mouth_left': (157, 129), 'mouth_right': (181, 128)}}]\n",
      "[]\n",
      "[{'box': [135, 61, 69, 80], 'confidence': 0.9999568462371826, 'keypoints': {'left_eye': (157, 96), 'right_eye': (189, 96), 'nose': (174, 119), 'mouth_left': (159, 128), 'mouth_right': (185, 128)}}]\n",
      "[]\n",
      "[{'box': [144, 59, 69, 80], 'confidence': 0.9998548030853271, 'keypoints': {'left_eye': (165, 94), 'right_eye': (197, 98), 'nose': (179, 120), 'mouth_left': (163, 125), 'mouth_right': (190, 127)}}]\n",
      "[]\n",
      "[{'box': [153, 71, 73, 79], 'confidence': 0.9990301132202148, 'keypoints': {'left_eye': (177, 106), 'right_eye': (209, 109), 'nose': (191, 132), 'mouth_left': (175, 135), 'mouth_right': (202, 136)}}]\n",
      "[]\n",
      "[{'box': [144, 105, 55, 58], 'confidence': 0.7745946645736694, 'keypoints': {'left_eye': (161, 126), 'right_eye': (177, 125), 'nose': (166, 142), 'mouth_left': (164, 149), 'mouth_right': (177, 148)}}]\n",
      "[]\n",
      "[{'box': [131, 84, 70, 70], 'confidence': 0.9991247057914734, 'keypoints': {'left_eye': (148, 118), 'right_eye': (175, 117), 'nose': (158, 142), 'mouth_left': (154, 146), 'mouth_right': (177, 145)}}]\n",
      "[]\n",
      "[{'box': [131, 71, 73, 80], 'confidence': 0.9999085664749146, 'keypoints': {'left_eye': (152, 104), 'right_eye': (186, 105), 'nose': (168, 130), 'mouth_left': (155, 137), 'mouth_right': (180, 137)}}]\n",
      "[]\n",
      "[{'box': [133, 65, 70, 81], 'confidence': 0.9999266862869263, 'keypoints': {'left_eye': (153, 100), 'right_eye': (188, 101), 'nose': (170, 125), 'mouth_left': (157, 132), 'mouth_right': (181, 133)}}]\n",
      "[]\n",
      "[{'box': [131, 72, 69, 77], 'confidence': 0.9999185800552368, 'keypoints': {'left_eye': (153, 106), 'right_eye': (187, 106), 'nose': (170, 130), 'mouth_left': (155, 136), 'mouth_right': (180, 137)}}]\n",
      "[]\n",
      "[{'box': [131, 87, 62, 63], 'confidence': 0.9731359481811523, 'keypoints': {'left_eye': (152, 117), 'right_eye': (172, 113), 'nose': (161, 134), 'mouth_left': (158, 139), 'mouth_right': (174, 136)}}]\n",
      "[]\n",
      "[{'box': [128, 108, 49, 53], 'confidence': 0.8190646767616272, 'keypoints': {'left_eye': (144, 125), 'right_eye': (160, 124), 'nose': (149, 142), 'mouth_left': (149, 150), 'mouth_right': (163, 149)}}]\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "dsk_image = face_recognition.load_image_file(\"dsk.jpg\")\n",
    "dsk_face_encoding = face_recognition.face_encodings(dsk_image)[0]\n",
    "\n",
    "    \n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    dsk_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"dsk\"\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        print(face_locations)\n",
    "        faces = detector.detect_faces(rgb_small_frame)\n",
    "        print(faces)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
